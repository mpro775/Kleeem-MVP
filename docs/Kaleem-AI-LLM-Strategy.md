# استراتيجية إدارة نماذج الذكاء (LLM) في **Kaleem AI**
**آخر تحديث:** 2025-10-03 00:44 

> هذه الوثيقة تعتمد **Gemini 2.5 Flash‑Lite** كنموذج افتراضي (Paid Tier)، مع **AI‑Gateway** يفرض حصصًا (Quota) لكل تاجر، ومفاتيح **API** مُقيّدة لكل تاجر داخل مشروع واحد (مؤقتًا). على المدى المتوسط نضيف **vLLM + Llama 3.1 8B** على GPU ‏24GB لأسئلة الـFAQ، وعلى المدى الأبعد ننتقل إلى **Project‑per‑Merchant** لعزل الحصص والفوترة.

---

## 1) لماذا اخترنا هذا المسار؟
- **كلفة منخفضة جدًا/استجابة سريعة**: Flash‑Lite هو الأرخص في عائلة Gemini مع سقوف مرتفعة (RPM/TPM) تكفي عشرات المحادثات المتزامنة.
- **مرونة نمو تدريجي**: نبدأ بمشروع واحد + مفاتيح مقيّدة للتجار، نراقب استهلاك كل تاجر عبر الـ Gateway، ثم ننتقل لاحقًا إلى Project‑per‑Merchant عند الحاجة لعزل الحصص والفوترة.
- **تقليل المخاطر**: البوابة تمنع تسرب المفاتيح، تضع **Rate‑Limit/Quota** لكل تاجر، وتطبق **PII redaction** وحراسة جودة (Guardrails).

---

## 2) السعة الفعلية (Capacity) باستخدام **Gemini 2.5 Flash‑Lite**
> **قيود الخدمة تُطبّق على مستوى الـ Project وليس المفتاح**. لذلك وجود مفاتيح متعددة داخل نفس المشروع لن يرفع السقف، بل يسهل الإبطال والتتبع فقط.

- **Tier 1 (مدفوع)**: **4000 RPM** ≈ **66.7 RPS**، و**4,000,000 TPM** (tokens/minute).  
- **Tier 2**: 10,000 RPM و10M TPM.  
- **Tier 3**: حتى 30,000 RPM و30M TPM.

### ماذا يعني ذلك لسيناريو “10 تجّار × محادثة نشِطة الآن”؟
- نفترض لكل تاجر **رسالة كل 4 ثوانٍ** → **0.25 RPS**.  
- الإجمالي = **10 × 0.25 = 2.5 RPS**  ≪  **66.7 RPS** (Tier 1).  
- حتى الذروات اللحظية (10 رسائل في نفس الثانية) تقع ضمن السقف. الاختناق المحتمل يكون فقط لو **سياق الرسائل طويل جدًا** ويقترب من حد **TPM**.

> **خلاصة:** استخدام مشروع واحد مع Tier 1 كافٍ تمامًا لبداية الإنتاج حتى مع 10 تجّار نشطين بالتوازي.

---

## 3) التكلفة بالرموز (Tokens) — أمثلة عملية
**أسعار Flash‑Lite (Paid Tier):**  
- **إدخال:** **$0.075 لكل 1,000,000 توكن**  
- **إخراج:** **$0.30 لكل 1,000,000 توكن**

> **ملاحظة:** عدّ التوكنات تقديري (يعتمد على طول الرسالة والسياق). الأمثلة التالية عملية للموازنة وليست فاتورة رسمية.

| السيناريو | إدخال (in) | إخراج (out) | تكلفة الإدخال | تكلفة الإخراج | **التكلفة/رد** | **لكل 1,000 رد** |
|---|---:|---:|---:|---:|---:|---:|
| **قصير (FAQ)** | 300 | 150 | 300/1e6×0.075 = **$0.0000225** | 150/1e6×0.30 = **$0.0000450** | **$0.0000675** | **$0.0675** |
| **متوسط** | 800 | 400 | $0.0000600 | $0.0001200 | **$0.0001800** | **$0.1800** |
| **ثقيل** | 2000 | 1000 | $0.0001500 | $0.0003000 | **$0.0004500** | **$0.4500** |

### مثال رسائل (تقريبي للتوكن)
- **المستخدم:** “هل تشحنون إلى جدة؟ كم المدة والتكلفة؟”  (~**20–40** توكن).  
- **السياق/النظام:** تعليمات + مقتطفات سياسة الشحن (RAG) (~**200–400** توكن).  
- **الرد:** “نعم، نوفر شحنًا إلى جدة خلال 2–4 أيام عمل…”  (~**80–150** توكن).  
> هذه الأرقام تقع ضمن **سيناريو قصير/متوسط** أعلاه.

---

## 4) آلية المفاتيح والحصص الآن (Project واحد + مفاتيح لكل تاجر)
- عند إنشاء تاجر جديد: ننادي **API Keys API** لإنشاء مفتاح **مُقيَّد** على خدمة **Generative Language API**، ونوسم المفتاح بـ `merchantId`.  
- المفتاح **لا يُعطى للتاجر**، بل يُخزّن في **Vault**، ويُستخدم داخليًا عبر **AI‑Gateway**.  
- لأن السقف **per‑project**، فإن **الـ Gateway** يفرض حصصًا داخلية لكل تاجر:

**سياسة الحصص المقترحة (قابلة للتعديل):**
- **Requests/Day**: مبدئيًا 2,000–10,000 حسب الخطة.  
- **Tokens/Month**: 20–200 مليون توكن.  
- **Cost/Month**: سقف ثابت (مثلاً $10/$50/$200).  
- عند 80%/90% من الاستهلاك → **تنبيه**؛ فوق السقف → **خفض جودة** (نموذج أرخص/رد موجز) أو **إيقاف مؤقت** حتى الشحن/الترقية.

**سير التنفيذ (Sequence مختصر):**
1) Channel → **Backend** `/ingest/message`  
2) Backend يقرر: يحتاج ذكاء؟ إن نعم → **n8n** (RAG/LLM) عبر **AI‑Gateway**  
3) AI‑Gateway: `assertQuota(merchantId)` → `route(model)` → `invoke` → `recordUsage`  
4) Backend يرسل الرد للقناة + يسجل Analytics/Billing

---

## 5) المدى المتوسط — نماذج محلّية لتقليل التكلفة (Hybrid)
**الهدف:** تفريغ أسئلة الـFAQ/الملخصات إلى نموذج محلّي، والإبقاء على الحالات الثقيلة (تحليل/منطق معقّد) على Gemini.

**الاقتراح الفني:**
- **vLLM** كمحرّك تشغيل (Throughput ممتاز + API متوافق مع OpenAI).
- **نموذج:** **Llama 3.1 8B Instruct** أو **Qwen2.5‑7B Instruct** (جودة عربية جيدة).
- **عتاد مناسب (خادم واحد):**  
  - **GPU 24GB VRAM** (A10G / RTX 4090 / RTX 6000)،  
  - **RAM 32–64GB**، **8 vCPU+**، **NVMe**.  
- **ضبط مهم:** تقليل **context length** و**KV‑cache** للسيطرة على VRAM، واستخدام **FP16/FP8** أو **كوانتايز 4‑بت** عند الحاجة.

**فكرة التوجيه (Router):**
- **Low‑cost route:** FAQ + إجابات قصيرة → Local (vLLM)  
- **High‑accuracy route:** استدلال معقّد/أسئلة حسّاسة → Gemini

> بهذه الهجين، ينخفض متوسط تكلفة كل رد بشكل ملحوظ مع المحافظة على الجودة حيث يلزم.

---

## 6) مزوّدو GPU وأسعار شهرية تقديرية
> الأسعار بالساعة تتغيّر حسب المنطقة ونوع العقد؛ التقدير الشهري يفترض **720 ساعة/شهر** (تشغيل دائم).

| المزوّد | نوع الـGPU (≈24GB) | **$/ساعة** | **$/شهر (تقريب)** | ملاحظات |
|---|---|---:|---:|---|
| **RunPod** | **RTX 4090 (24GB)** | ~**0.59** (توجد عروض من ~0.34) | **$425** | أرخصية شائعة للاستخدام اللحظي/الاختباري. |
| **Lambda Labs** | **A10 (24GB)** | **0.75** | **$540** | توازن جيد سعر/ثبات. |
| **AWS EC2** | **g5.xlarge (A10G 24GB)** | **1.006** | **$724** | تكامل مؤسسي، شبكة وخدمات AWS. |
| **Paperspace** | **A5000 (24GB)** | **1.38** | **$994** | بديل بسيط، لكن أعلى سعرًا غالبًا. |

> **ملاحظة تشغيلية:** إن لم تكن بحاجة لتشغيل دائم، فكر في **Auto‑scale** مع توقيف عند الخمول لتقليل الفاتورة.

---

## 7) المدى الأبعد — **Project‑per‑Merchant**
**لماذا؟** عزل حصص وفوترة لكل تاجر، وإمكانية رفع السقوف لكل مشروع بشكل مستقل، وإبطال المخالفين دون المساس بالآخرين.

**كيف؟**
1) عند إنشاء تاجر: ننشئ **GCP Project** جديدًا تلقائيًا ونربطه بـ **Cloud Billing**.  
2) نفعّل **Generative Language API** للمشروع، وننشئ **API Key** مقيّد.  
3) بوابتنا تستخدم مفتاح التاجر الخاص به—وبذلك **القيود/السقوف تصبح لكل تاجر** على مستوى جوجل.

**متى ننتقل؟**
- عندما نحتاج **عزل حصص/فوترة حقيقي** (تجار كبار/متطلبات امتثال)، أو عند اقترابنا عمليًا من سقوف مشروع واحد.

---

## 8) متطلبات جوجل (الحساب والفوترة والـTiers)
- للانتقال من Free إلى Paid: **تفعيل Cloud Billing** للمشروع وربطه بوسيلة دفع صالحة.  
- **Tier 1:** مشروع مرتبط بالفوترة.  
- **Tier 2:** إنفاق تراكمي > **$250** ومرّ **30 يومًا** منذ أول دفعة ناجحة.  
- **Tier 3:** إنفاق تراكمي > **$1000** ومرّ **30 يومًا** منذ أول دفعة ناجحة.  
- رفع الحدود يتم عبر طلب ترقية/زيادة من صفحة **Rate Limits/Quotas**.

> لا يُطلب “كيان قانوني خاص” لبدء الاستخدام الذاتي (Self‑serve) عادة؛ لكن **الفوترة بالفاتورة (Invoiced billing)** أو عقود Enterprise قد تتطلّب موافقات/فحوصات ائتمانية واتفاقية.

---

## 9) المخاطر والتخفيف
- **انفجار تكلفة/حصص:** حصص صارمة per‑merchant + تنبيهات 60%/80%/95% + Fallback لنموذج أرخص.  
- **تسرب PII:** PII‑redaction قبل الإرسال + ماسكات في اللوغز + تشفير الأسرار.  
- **انقطاع مزوّد:** مزوّد بديل (OpenAI/Anthropic أو Local) + **Circuit‑Breaker**.  
- **هلاوس على الأسعار/السياسات:** Guardrails + تحقق بعدي (Post‑Checks) على الحقول الحساسة.  
- **ضغط مفاجئ:** Backpressure/Queue + “قيد المعالجة” ثم رد لاحق Event‑Driven.

---

## 10) ملحق — صيغ/معادلات مفيدة
- **تحويل RPM → RPS:** `RPS ≈ RPM / 60`. مثال: 4000 RPM → **66.7 RPS**.  
- **تكلفة الإدخال:** `tokens_in / 1e6 × 0.075 $`.  
- **تكلفة الإخراج:** `tokens_out / 1e6 × 0.30 $`.  
- **تكلفة 1000 رد:** `cost_per_reply × 1000`.  
- **سعة TPM:** `الحد TPM ÷ متوسط tokens_in` ≈ **عدد الردود الممكنة/دقيقة** (تقريبي).

---

## 11) خطوات تنفيذية مختصرة (Ready‑To‑Apply)
1) تفعيل **Paid Tier** للمشروع.  
2) نشر **AI‑Gateway** (NestJS) بقدرات: `assertQuota`, `route(model)`, `invoke`, `recordUsage`.  
3) إنشاء مفاتيح **API** مقيّدة لكل تاجر عبر **API Keys API** وتخزينها في **Vault**.  
4) تحديث وركفلو **n8n** لاستدعاء البوابة بدلًا من عقدة الـ LLM المباشرة.  
5) إضافة **Grafana Dashboards** + تنبيهات استهلاك وكلفة per‑merchant.  
6) PoC لنموذج **vLLM + Llama 3.1 8B** على GPU ‏24GB؛ اختبار Router (Local vs Gemini).  
7) مراجعة فصل **Project‑per‑Merchant** بعد أول شهرين من التشغيل بناءً على البيانات.

---

### خلاصة تنفيذية
ابدأ الآن بـ **Gemini Flash‑Lite (Paid Tier)** + **AI‑Gateway** يفرض الحصص لكل تاجر مع مفاتيح مقيّدة داخل مشروع واحد. أضِف نموذجًا محليًا عبر **vLLM** لتقليل تكاليف الأسئلة البسيطة. ومع نمو الحمل أو الحاجة لعزل فوترة/حصص حقيقي، انتقل تدريجيًا إلى **Project‑per‑Merchant**.

---

### (اختياري) مراجع
> تم الرجوع إلى صفحات التسعير والحدود الرسمية من Google AI، وإلى صفحات مزودي GPU الشائعة، بالإضافة إلى توثيق/نقاشات حول متطلبات VRAM وتشغيل vLLM. يُنصح بالاطلاع على الروابط الرسمية قبل الشراء لحدوث تغييرات مستمرة في الأسعار/الحدود.
